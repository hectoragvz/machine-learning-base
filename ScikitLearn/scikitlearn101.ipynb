{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Introduction\n",
    "\n",
    "What is covered in the following notebook:\n",
    "\n",
    "1. Getting the data ready\n",
    "2. Picking a model to suit your problem\n",
    "3. Fit the model to the data and make a prediction\n",
    "4. Evaluating the model\n",
    "5. Improve through experimentation\n",
    "6. Save and reload your trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Standard Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 2 datasets for demonstration purposes.\n",
    "\n",
    "* heart_disease - a classification dataset (predicting whether someone has heart disease or not)\n",
    "* boston_df - a regression dataset (predicting the median house prices of cities in Boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification data\n",
    "heart_disease = pd.read_csv(\"./heart-disease.csv\")\n",
    "\n",
    "# Regression Data\n",
    "# After installing scikit learn - pip install -U scikit-learn\n",
    "# Since the boston dataset has been recently removed, googling around we can find the dataset via this way:\n",
    "from sklearn.datasets import fetch_openml\n",
    "boston = fetch_openml(data_id=506)\n",
    "\n",
    "# Convert dictionary to dataframe\n",
    "boston_df = pd.DataFrame(boston[\"data\"], columns=boston[\"feature_names\"])\n",
    "boston_df[\"target\"] = pd.Series(boston[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the Data Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X & y\n",
    "X = heart_disease.drop(\"target\", axis=1) # use all columns except target\n",
    "y = heart_disease[\"target\"] # we want to predict y using X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Example use case (requires X & y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pick a model/estimator (to suit your problem)\n",
    "\n",
    "We can start our model selection via the [Scikit Learn cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html).\n",
    "\n",
    "The greatest ML knowledge base lies under knowing the math foundations for most of the models in here. However, this is a high level overview of the use of the library, learning the tradeoffs between each model is the difference between many practitioners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier (for classification problems)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Instantiating a Random Forest Classifier (clf short for classifier)\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Random Forest Regressor (for regression problems)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiating a Random Forest Regressor\n",
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fit the Model to the Data and Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1], dtype=int64),\n",
       " array([[0.4 , 0.6 ],\n",
       "        [0.15, 0.85],\n",
       "        [0.37, 0.63],\n",
       "        [0.34, 0.66],\n",
       "        [0.03, 0.97],\n",
       "        [0.97, 0.03],\n",
       "        [0.5 , 0.5 ],\n",
       "        [0.29, 0.71],\n",
       "        [0.23, 0.77],\n",
       "        [1.  , 0.  ],\n",
       "        [0.01, 0.99],\n",
       "        [0.73, 0.27],\n",
       "        [0.04, 0.96],\n",
       "        [0.27, 0.73],\n",
       "        [0.8 , 0.2 ],\n",
       "        [0.23, 0.77],\n",
       "        [0.39, 0.61],\n",
       "        [0.12, 0.88],\n",
       "        [0.63, 0.37],\n",
       "        [0.56, 0.44],\n",
       "        [0.84, 0.16],\n",
       "        [0.14, 0.86],\n",
       "        [0.57, 0.43],\n",
       "        [0.11, 0.89],\n",
       "        [0.07, 0.93],\n",
       "        [0.24, 0.76],\n",
       "        [0.81, 0.19],\n",
       "        [0.67, 0.33],\n",
       "        [0.45, 0.55],\n",
       "        [0.61, 0.39],\n",
       "        [0.57, 0.43],\n",
       "        [0.64, 0.36],\n",
       "        [0.17, 0.83],\n",
       "        [0.72, 0.28],\n",
       "        [0.21, 0.79],\n",
       "        [0.76, 0.24],\n",
       "        [0.28, 0.72],\n",
       "        [0.79, 0.21],\n",
       "        [0.37, 0.63],\n",
       "        [0.76, 0.24],\n",
       "        [0.37, 0.63],\n",
       "        [0.31, 0.69],\n",
       "        [0.82, 0.18],\n",
       "        [0.86, 0.14],\n",
       "        [0.58, 0.42],\n",
       "        [0.1 , 0.9 ],\n",
       "        [0.98, 0.02],\n",
       "        [0.9 , 0.1 ],\n",
       "        [0.83, 0.17],\n",
       "        [0.47, 0.53],\n",
       "        [0.56, 0.44],\n",
       "        [0.3 , 0.7 ],\n",
       "        [0.92, 0.08],\n",
       "        [0.89, 0.11],\n",
       "        [0.59, 0.41],\n",
       "        [0.72, 0.28],\n",
       "        [0.03, 0.97],\n",
       "        [0.99, 0.01],\n",
       "        [0.15, 0.85],\n",
       "        [0.35, 0.65],\n",
       "        [0.21, 0.79],\n",
       "        [0.9 , 0.1 ],\n",
       "        [0.04, 0.96],\n",
       "        [0.42, 0.58],\n",
       "        [0.03, 0.97],\n",
       "        [0.47, 0.53],\n",
       "        [0.56, 0.44],\n",
       "        [0.05, 0.95],\n",
       "        [0.41, 0.59],\n",
       "        [0.52, 0.48],\n",
       "        [0.84, 0.16],\n",
       "        [0.07, 0.93],\n",
       "        [0.23, 0.77],\n",
       "        [0.01, 0.99],\n",
       "        [0.86, 0.14],\n",
       "        [0.29, 0.71]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All models/estimators have the fit() function built-in\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Once fit is called, you can make predictions using predict()\n",
    "y_preds = clf.predict(X_test)\n",
    "\n",
    "# You can also predict with probabilities (on classification models)\n",
    "y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "# View preds/probabilities\n",
    "y_preds, y_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating a Model\n",
    "\n",
    "Every Scikit-Learn model has a default metric which is accessible through the score() function.\n",
    "\n",
    "However there are a range of different evaluation metrics you can use depending on the model you're using.\n",
    "\n",
    "A full list of evaluation metrics can be found in the [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8552631578947368"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All models/estimators have a score() function\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85245902 0.86885246 0.80327869 0.78333333 0.75      ]\n",
      "[0.8125     0.90625    0.84375    0.81818182 0.74358974]\n"
     ]
    }
   ],
   "source": [
    "# Evaluting a model using cross-validation is possible with cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# scoring=None means default score() metric is used\n",
    "print(\n",
    "    cross_val_score(\n",
    "        estimator=clf, X=X, y=y, cv=5, scoring=None  # use 5-fold cross-validation\n",
    "    )\n",
    ")\n",
    "\n",
    "# Evaluate a model with a different scoring method\n",
    "print(\n",
    "    cross_val_score(\n",
    "        estimator=clf,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=5,  # use 5-fold cross-validation\n",
    "        scoring=\"precision\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8552631578947368\n",
      "0.8541666666666666\n",
      "[[30  6]\n",
      " [ 5 35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85        36\n",
      "           1       0.85      0.88      0.86        40\n",
      "\n",
      "    accuracy                           0.86        76\n",
      "   macro avg       0.86      0.85      0.85        76\n",
      "weighted avg       0.86      0.86      0.86        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Different classification metrics\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "\n",
    "# Reciver Operating Characteristic (ROC curve)/Area under curve (AUC)\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probs[:, 1])\n",
    "print(roc_auc_score(y_test, y_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_preds))\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.6271535803941815\n",
      "0.5377061555261198\n",
      "0.5060949228556136\n"
     ]
    }
   ],
   "source": [
    "# Different regression metrics\n",
    "\n",
    "# Make predictions first\n",
    "X = boston_df.drop(\"target\", axis=1)\n",
    "y = boston_df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# R^2 (pronounced r-squared) or coefficient of determination\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(y_test, y_preds))\n",
    "\n",
    "# Mean absolute error (MAE)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(y_test, y_preds))\n",
    "\n",
    "# Mean square error (MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improve through experimentation\n",
    "\n",
    "Two of the main methods to improve a models baseline metrics (the first evaluation metrics you get).\n",
    "\n",
    "From a data perspective asks:\n",
    "\n",
    "Could we collect more data? \n",
    "\n",
    "In machine learning, more data is generally better, as it gives a model more opportunities to learn patterns.\n",
    "Could we improve our data? \n",
    "\n",
    "This could mean filling in misisng values or finding a better encoding (turning things into numbers) strategy.\n",
    "\n",
    "From a model perspective asks:\n",
    "\n",
    "Is there a better model we could use? \n",
    "\n",
    "If you've started out with a simple model, could you use a more complex one? (we saw an example of this when looking at the Scikit-Learn machine learning map, ensemble methods are generally considered more complex models)\n",
    "\n",
    "Could we improve the current model? \n",
    "\n",
    "If the model you're using performs well straight out of the box, can the hyperparameters be tuned to make it even better?\n",
    "\n",
    "Hyperparameters are like settings on a model you can adjust so some of the ways it uses to find patterns are altered and potentially improved. Adjusting hyperparameters is referred to as hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'monotonic_cst': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How to find a model's hyperparameters\n",
    "clf = RandomForestClassifier()\n",
    "clf.get_params() # returns a list of adjustable hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7631578947368421\n",
      "0.7631578947368421\n"
     ]
    }
   ],
   "source": [
    "# Example of adjusting hyperparameters by hand\n",
    "\n",
    "# Split data into X & y\n",
    "X = heart_disease.drop(\"target\", axis=1) # use all columns except target\n",
    "y = heart_disease[\"target\"] # we want to predict y using X\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Instantiate two models with different settings\n",
    "clf_1 = RandomForestClassifier(n_estimators=100)\n",
    "clf_2 = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit both models on training data\n",
    "clf_1.fit(X_train, y_train)\n",
    "clf_2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate both models on test data and see which is best\n",
    "print(clf_1.score(X_test, y_test))\n",
    "print(clf_2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   2.2s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   2.2s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   2.3s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   2.2s\n",
      "[CV] END max_depth=5, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   2.2s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=4, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=1, min_samples_split=6, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=5, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=500; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=None, max_features=auto, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=10, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   0.0s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   0.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   0.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   0.8s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   0.9s\n",
      "[CV] END max_depth=30, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=500; total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\usuario\\Documents\\ML\\machine-learning-base\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:542: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\usuario\\Documents\\ML\\machine-learning-base\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 890, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\usuario\\Documents\\ML\\machine-learning-base\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1344, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\usuario\\Documents\\ML\\machine-learning-base\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\usuario\\Documents\\ML\\machine-learning-base\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\usuario\\Documents\\ML\\machine-learning-base\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.81445578 0.82678571        nan 0.80204082 0.81445578        nan\n",
      "        nan        nan        nan 0.81862245]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of adjusting hyperparameters computationally (recommended)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters\n",
    "grid = {\n",
    "    \"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "    \"max_depth\": [None, 5, 10, 20, 30],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"],\n",
    "    \"min_samples_split\": [2, 4, 6],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Set n_jobs to -1 to use all cores (NOTE: n_jobs=-1 is broken as of 8 Dec 2019, using n_jobs=1 works)\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "rs_clf = RandomizedSearchCV(\n",
    "    estimator=clf,\n",
    "    param_distributions=grid,\n",
    "    n_iter=10,  # try 10 models total\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=2,\n",
    ")  # print out results\n",
    "\n",
    "# Fit the RandomizedSearchCV version of clf\n",
    "rs_clf.fit(X_train, y_train)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "print(rs_clf.best_params_)\n",
    "\n",
    "# Scoring automatically uses the best hyperparameters\n",
    "rs_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save and Reload a Model\n",
    "\n",
    "You can save and load a model with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a model with pickle\n",
    "import pickle\n",
    "\n",
    "# Save an existing model to file\n",
    "pickle.dump(rs_clf, open(\"rs_random_forest_model_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a saved pickle model\n",
    "loaded_pickle_model = pickle.load(open(\"rs_random_forest_model_1.pkl\", \"rb\"))\n",
    "\n",
    "# Evaluate loaded model\n",
    "loaded_pickle_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the same with joblib. joblib is usually more efficient with numerical data (what our models are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs_random_forest_model_1.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving a model with joblib\n",
    "from joblib import dump, load\n",
    "\n",
    "# Save a model to file\n",
    "dump(rs_clf, filename=\"gs_random_forest_model_1.joblib\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a saved joblib model\n",
    "loaded_joblib_model = load(filename=\"gs_random_forest_model_1.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524590163934426"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate joblib predictions \n",
    "loaded_joblib_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Putting it all together (not pictured)\n",
    "\n",
    "We can put a number of different Scikit-Learn functions together using Pipeline.\n",
    "\n",
    "As an example, we'll use car-sales-extended-missing-data.csv. Which has missing data as well as non-numeric data. For a machine learning model to work, there can be no missing data or non-numeric values.\n",
    "\n",
    "The problem we're solving here is predicting a cars sales price given a number of parameters about the car (a regression problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22188417408787875"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting data ready\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Setup random seed\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import data and drop the rows with missing labels\n",
    "data = pd.read_csv(\"./car-sales-extended-missing-data.csv\")\n",
    "data.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "# Define different features and transformer pipelines\n",
    "categorical_features = [\"Make\", \"Colour\"]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "door_feature = [\"Doors\"]\n",
    "door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))])\n",
    "\n",
    "numeric_features = [\"Odometer (KM)\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n",
    "])\n",
    "\n",
    "# Setup preprocessing steps (fill missing values, then convert to numbers)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "        (\"door\", door_transformer, door_feature),\n",
    "        (\"num\", numeric_transformer, numeric_features)])\n",
    "\n",
    "# Create a preprocessing and modelling pipeline\n",
    "model = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "                        (\"model\", RandomForestRegressor())])\n",
    "\n",
    "# Split data\n",
    "X = data.drop(\"Price\", axis=1)\n",
    "y = data[\"Price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fit and score the model\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
